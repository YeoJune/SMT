# Example SMT Experiment Configuration
# Minimal setup for quick testing
# Run with: python .\scripts\run_experiment.py --config configs\example_experiment.yaml

# ============================================================================
# Experiment Metadata
# ============================================================================
experiment:
  name: example_smt
  seed: 42
  output_dir: experiments/results/example_smt

# ============================================================================
# Model Architecture Configuration
# ============================================================================
model:
  model_type: stride_memory_transformer

  # Vocabulary
  vocab_size: 50257
  pad_token_id: 50256

  # Core dimensions (smaller for testing)
  d_model: 256
  dropout: 0.1
  max_position_embeddings: 512

  # Window configuration
  window:
    n_memory_tokens: 8 # Fewer memory tokens for testing
    n_input_tokens: 24 # Smaller input buffer
    stride: 8 # Update every 8 tokens

  # Transformer configuration
  transformer:
    num_layers: 6 # Fewer layers for speed
    num_attention_heads: 4
    intermediate_size: 1024
    hidden_dropout_prob: 0.1
    attention_dropout_prob: 0.1
    pretrained_model: null # Random init for testing

  # SSM configuration
  ssm:
    num_layers: 6
    state_size: 16
    conv_kernel: 4
    expand_factor: 2
    pretrained_model: null # Random init for testing

  # Attention Pooling
  attention_pooling:
    query_dim: 256

# ============================================================================
# Dataset Configuration
# ============================================================================
data:
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1
  max_seq_length: 128 # Shorter sequences for testing

  train_batch_size: 4
  eval_batch_size: 4
  num_workers: 0 # 0 for debugging
  pin_memory: false

# ============================================================================
# Training Configuration
# ============================================================================
training:
  max_steps: 100 # Just 100 steps for testing
  gradient_accumulation_steps: 1

  learning_rate: 1.0e-3
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  lr_scheduler_type: linear
  warmup_steps: 10

  save_strategy: steps
  save_steps: 50
  save_total_limit: 2

  evaluation_strategy: steps
  eval_steps: 50

  fp16: false
  bf16: false

# ============================================================================
# Evaluation Configuration
# ============================================================================
evaluation:
  max_eval_batches: 10
  eval_on_start: false

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  logging_steps: 10
  report_to: []
  wandb_project: smt-example
  wandb_run_name: null
  log_level: info
