# WikiText-2 Language Modeling Experiment
# Run with: python .\scripts\run_experiment.py --config configs\wikitext2_experiment.yaml

# ============================================================================
# Experiment Metadata
# ============================================================================
experiment:
  name: wikitext2_smt
  seed: 42
  output_dir: experiments/results/wikitext2_smt

# ============================================================================
# Model Architecture Configuration
# ============================================================================
model:
  # Model type
  model_type: stride_memory_transformer

  # Vocabulary
  vocab_size: 50257 # GPT-2 tokenizer
  pad_token_id: 50256 # GPT-2 EOS as padding

  # Core dimensions
  d_model: 768
  dropout: 0.1
  max_position_embeddings: 1024

  # Memory optimization
  use_gradient_checkpointing: true # Enable gradient checkpointing
  use_amp: true # Enable automatic mixed precision (FP16)

  # Window configuration (Stride Memory specific)
  window:
    n_memory_tokens: 16 # Number of SSM memory tokens (n)
    n_input_tokens: 48 # Number of recent input tokens (m)
    stride: 16 # Update frequency (m/4)

  # Transformer configuration
  transformer:
    num_layers: 2
    num_attention_heads: 12
    intermediate_size: 3072 # Feed-forward dimension (d_ff)
    hidden_dropout_prob: 0.1
    attention_dropout_prob: 0.1
    pretrained_model: gpt2 # or null for random init

  # SSM (Mamba) configuration
  ssm:
    num_layers: 6
    state_size: 16 # d_state
    conv_kernel: 4 # d_conv
    expand_factor: 2
    pretrained_model: state-spaces/mamba-130m-hf # or null

  # Attention Pooling configuration
  attention_pooling:
    query_dim: 768 # d_k for pooling attention

# ============================================================================
# Dataset Configuration
# ============================================================================
data:
  # Dataset source
  dataset_name: wikitext
  dataset_config: wikitext-2-raw-v1 # or wikitext-2-v1 for tokenized

  # Preprocessing
  max_seq_length: 512

  # DataLoader settings
  train_batch_size: 8
  eval_batch_size: 8
  num_workers: 2
  pin_memory: true

# ============================================================================
# Training Configuration
# ============================================================================
training:
  # Training steps
  max_steps: 10000
  gradient_accumulation_steps: 1

  # Optimization
  learning_rate: 5.0e-4
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1.0e-8
  max_grad_norm: 1.0

  # Learning rate schedule
  lr_scheduler_type: linear # linear, cosine, constant
  warmup_steps: 500

  # Checkpointing
  save_strategy: steps
  save_steps: 1000
  save_total_limit: 3

  # Evaluation
  evaluation_strategy: steps
  eval_steps: 500

  # Mixed precision
  fp16: false
  bf16: false

# ============================================================================
# Evaluation Configuration
# ============================================================================
evaluation:
  max_eval_batches: 100 # Limit for faster evaluation during training
  eval_on_start: false

# ============================================================================
# Logging Configuration
# ============================================================================
logging:
  # Console logging
  logging_steps: 100

  # Weights & Biases
  report_to: [] # ["wandb"] to enable
  wandb_project: smt-wikitext2
  wandb_run_name: null # Auto-generated if null

  # Log level
  log_level: info # debug, info, warning, error
