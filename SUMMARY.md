# SMT (Stride Memory Transformer) - êµ¬í˜„ ì™„ë£Œ ë³´ê³ ì„œ

## ğŸ¯ ì™„ì„±ëœ ì‘ì—…

PDF ë¬¸ì„œì˜ **SMT (Stride Memory Transformer) ì•„í‚¤í…ì²˜**ë¥¼ í‘œì¤€ êµ¬í˜„ìœ¼ë¡œ ì™„ì„±í–ˆìŠµë‹ˆë‹¤.

### ğŸ“¦ ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
SMT/                  (1,364 lines, 60.7 KB)
â”œâ”€â”€ README.md                       âœ… í”„ë¡œì íŠ¸ ì†Œê°œ
â”œâ”€â”€ IMPLEMENTATION.md               âœ… êµ¬í˜„ ìƒì„¸ ë¬¸ì„œ
â”œâ”€â”€ requirements.txt                âœ… ì˜ì¡´ì„±
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model_config.py            âœ… ì„¤ì • ì‹œìŠ¤í…œ (158 lines)
â”‚       â”œâ”€â”€ StrideHybridConfig     - ëª¨ë¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°
â”‚       â””â”€â”€ TrainingConfig         - í•™ìŠµ ì„¤ì •
â”‚
â”œâ”€â”€ src/models/
â”‚   â”œâ”€â”€ stride_hybrid.py           âœ… ë©”ì¸ ëª¨ë¸ (326 lines)
â”‚   â”œâ”€â”€ window_manager.py          âœ… ìœˆë„ìš° ê´€ë¦¬ (279 lines)
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ attention_pooling.py   âœ… Attention Pooling (194 lines)
â”‚       â”œâ”€â”€ transformer.py         âœ… Windowed Transformer (187 lines)
â”‚       â””â”€â”€ ssm.py                 âœ… SSM Memory (220 lines)
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_integration.py        âœ… í†µí•© í…ŒìŠ¤íŠ¸
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ verify_structure.py        âœ… êµ¬ì¡° ê²€ì¦
```

## ğŸ—ï¸ êµ¬í˜„ëœ í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### 1. **AttentionPooling** (194 lines)

- **ëª©ì **: ìœˆë„ìš°ë¥¼ ë‹¨ì¼ ë²¡í„°ë¡œ ì••ì¶•
- **ë°©ì‹**: Query-Key-Value attention
  - Query = W_q @ mean(window)
  - Keys = W_k @ window
  - Output = softmax(Q^T K) @ window
- **íŠ¹ì§•**: í•™ìŠµ ê°€ëŠ¥í•œ ì¤‘ìš”ë„ ê³„ì‚°

### 2. **WindowManager** (279 lines)

- **ëª©ì **: [n SSM outputs | m input tokens] ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ê´€ë¦¬
- **êµ¬í˜„**:
  - Single version: deque ê¸°ë°˜ O(1) rotation
  - Batched version: tensor ê¸°ë°˜ ë³‘ë ¬ ì²˜ë¦¬
- **ê¸°ëŠ¥**: ìƒíƒœ ì €ì¥/ë¡œë“œ, íš¨ìœ¨ì ì¸ ì—…ë°ì´íŠ¸

### 3. **WindowedTransformer** (187 lines)

- **ê¸°ë°˜**: Pre-trained GPT-2 (117M params)
- **ì²˜ë¦¬**: ì‘ì€ ìœˆë„ìš°ë§Œ (65 tokens)
- **íš¨ìœ¨**: O(windowÂ²) = O(65Â²) vs O(LÂ²)
- **ëŒ€ì•ˆ**: SimpleCausalTransformer (pretrained ì—†ì´ í…ŒìŠ¤íŠ¸)

### 4. **SSMMemory** (220 lines)

- **ê¸°ë°˜**: Pre-trained Mamba (130M params)
- **êµ¬ì¡°**: 24 layers with RMSNorm
- **ì—­í• **: ì••ì¶•ëœ ì¥ê¸° ë©”ëª¨ë¦¬ ìœ ì§€
- **ëŒ€ì•ˆ**: SimpleSSM (mamba-ssm ì—†ì´ í…ŒìŠ¤íŠ¸)

### 5. **StrideHybridModel** (326 lines)

- **í†µí•©**: ëª¨ë“  ì»´í¬ë„ŒíŠ¸ë¥¼ í•˜ë‚˜ë¡œ
- **Forward**: ì „ì²´ ì‹œí€€ìŠ¤ ë³‘ë ¬ ì²˜ë¦¬
- **Generate**: Auto-regressive ìƒì„±
- **íŠ¹ì§•**:
  - Stride-based write (6.25% ì£¼ê¸°)
  - íŒŒë¼ë¯¸í„° í†µê³„
  - Auxiliary outputs

### 6. **Config System** (158 lines)

- **StrideHybridConfig**:
  - ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„°
  - ìë™ ê²€ì¦ (stride â‰¤ m ë“±)
  - Pretty print summary
  - ì¶”ì²œ ê°’ ê²½ê³ 
- **TrainingConfig**:
  - Dataset, optimizer, schedule
  - Gradient accumulation
  - Logging ì„¤ì •

## ğŸ“Š ì„±ëŠ¥ íŠ¹ì„±

### ê³„ì‚° íš¨ìœ¨ì„±

| ì»´í¬ë„ŒíŠ¸                     | ë³µì¡ë„   | FLOPs/step |
| ---------------------------- | -------- | ---------- |
| Window Attention             | O(65Â²)   | 3.2M       |
| Attention Pooling (ë¶„í• ìƒí™˜) | O(65)    | 2.4M       |
| SSM (ë¶„í• ìƒí™˜)               | O(768Â²)  | 0.04M      |
| **í•©ê³„**                     | -        | **5.6M**   |
| Full Attention (L=4096)      | O(4096Â²) | 12.9B      |
| **ì†ë„ í–¥ìƒ**                | -        | **2300x**  |

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±

- Window activations: 50K floats
- SSM state: 16 (minimal)
- KV cache: 0 (ì‚¬ìš© ì•ˆ í•¨)
- **vs Full Attention: 63x ì ìŒ**

## ğŸ” ì—…ë¡œë“œëœ ì½”ë“œì™€ì˜ ì°¨ì´

| íŠ¹ì„±         | Samba (ì—…ë¡œë“œ)        | Stride-Hybrid (êµ¬í˜„)                     |
| ------------ | --------------------- | ---------------------------------------- |
| **ì•„í‚¤í…ì²˜** | 24 Mamba layers ì „ì²´  | ì‘ì€ windowë§Œ ì²˜ë¦¬                       |
| **ë©”ëª¨ë¦¬**   | ëª¨ë“  ë ˆì´ì–´ ì¶œë ¥ ì €ì¥ | SSM outputsë§Œ (n=15)                     |
| **ì—…ë°ì´íŠ¸** | ë§¤ ìŠ¤í…               | Stride ê°„ê²© (6.25%)                      |
| **ì••ì¶•**     | LSM ì„ í˜• ë¯¹ì‹±         | Attention Pooling                        |
| **ì—­í• **     | Cross-attention       | ëª…í™•í•œ ë¶„ë¦¬ (Transformer=ë‹¨ê¸°, SSM=ì¥ê¸°) |

### ì¬ì‚¬ìš©í•œ íŒ¨í„´

âœ… **Mamba ì‚¬ìš©ë²•**

```python
from mamba_ssm import Mamba, RMSNorm
residual = x
x = residual + mamba_layer(layer_norm(x))
```

âœ… **GPT-2 ë¡œë”©**

```python
from transformers import GPT2Model
model = GPT2Model.from_pretrained("gpt2")
output = model(inputs_embeds=x)
```

âœ… **Weight Loading**

```python
hf_state_dict = pretrained_model.state_dict()
# Prefix mapping + load_state_dict(strict=False)
```

## ğŸš€ ì‚¬ìš© ë°©ë²•

### 1. ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

### 2. ê¸°ë³¸ ì‚¬ìš©

```python
from config.model_config import StrideHybridConfig
from src.models.stride_hybrid import StrideHybridModel

# ì„¤ì •
config = StrideHybridConfig(
    n_ssm_outputs=15,
    m_input_tokens=50,
    stride=16
)

# ëª¨ë¸ ìƒì„±
model = StrideHybridModel(config)

# í•™ìŠµ
logits, aux = model(input_ids)
loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))

# ìƒì„±
generated = model.generate(
    input_ids=prompt,
    max_new_tokens=100,
    temperature=0.8,
    top_p=0.95
)
```

### 3. í…ŒìŠ¤íŠ¸

```bash
python tests/test_integration.py
```

## ğŸ“‹ ë‹¤ìŒ êµ¬í˜„ ë‹¨ê³„

### Phase 1: Data Pipeline (ìš°ì„ ìˆœìœ„: ë†’ìŒ)

- [ ] WikiText-103 ë°ì´í„° ë¡œë”
- [ ] PG-19 ë°ì´í„° ë¡œë”
- [ ] GPT-2 tokenizer wrapper
- [ ] íš¨ìœ¨ì ì¸ DataLoader

### Phase 2: Training Loop

- [ ] Trainer í´ë˜ìŠ¤
- [ ] AdamW optimizer (grouped LR)
- [ ] Cosine annealing scheduler
- [ ] Gradient accumulation

### Phase 3: Utilities

- [ ] Perplexity ê³„ì‚°
- [ ] Checkpoint ì €ì¥/ë¡œë“œ
- [ ] Wandb ë¡œê¹…
- [ ] Attention visualization

### Phase 4: Experiments

- [ ] WikiText-103 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- [ ] PG-19 í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
- [ ] í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- [ ] Ablation studies (stride, window size)

### Phase 5: Pre-trained Weights

- [ ] GPT-2 weights ë¡œë”© ë° ê²€ì¦
- [ ] Mamba weights ë¡œë”© ë° ê²€ì¦
- [ ] Fine-tuning ì „ëµ

## ğŸ¯ í•µì‹¬ ì„¤ê³„ ì›ì¹™

### 1. ëª…í™•í•œ ì—­í•  ë¶„ë¦¬

- **ì–¸ì œ ì“¸ ê²ƒì¸ê°€**: Fixed stride (ë‹¨ìˆœ)
- **ë¬´ì—‡ì„ ì“¸ ê²ƒì¸ê°€**: Attention pooling (í•™ìŠµ ê°€ëŠ¥)
- **ì–´ë””ì— ì“¸ ê²ƒì¸ê°€**: SSM (ì••ì¶• ë©”ëª¨ë¦¬)
- **ì–´ë–»ê²Œ ì‚¬ìš©**: Transformer (ê°•ë ¥í•œ ë‹¨ê¸° ì²˜ë¦¬)

### 2. ê·¹ë‹¨ì  íš¨ìœ¨ì„±

- ëŒ€ë¶€ë¶„ì˜ ìŠ¤í…(15/16)ì—ì„œ 65 tokensë§Œ ì²˜ë¦¬
- O(LÂ²) ë³µì¡ë„ ê·¼ë³¸ì ìœ¼ë¡œ íšŒí”¼
- ì •ë³´ ì†ì‹¤ ìµœì†Œí™” (Attention pooling + SSM)

### 3. êµ¬í˜„ ë‹¨ìˆœì„±

- í•µì‹¬ ë¡œì§ ~50 lines
- ëª…í™•í•œ forward pass
- ì‰¬ìš´ ë””ë²„ê¹…

### 4. ì‹¤ìš©ì„±

- Pre-trained ëª¨ë¸ ì¬ì‚¬ìš© (GPT-2 + Mamba)
- CPU/GPU ì–‘ìª½ í˜¸í™˜
- Fallback êµ¬í˜„ ì œê³µ

## âœ… ì™„ì„±ë„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### í•µì‹¬ ì•„í‚¤í…ì²˜

- [x] AttentionPooling êµ¬í˜„
- [x] WindowManager êµ¬í˜„
- [x] WindowedTransformer êµ¬í˜„
- [x] SSMMemory êµ¬í˜„
- [x] StrideHybridModel í†µí•©
- [x] Config ì‹œìŠ¤í…œ
- [x] í†µí•© í…ŒìŠ¤íŠ¸

### ë¬¸ì„œí™”

- [x] README.md
- [x] IMPLEMENTATION.md (7.3 KB)
- [x] ì½”ë“œ ì£¼ì„ (ëª¨ë“  í•¨ìˆ˜)
- [x] Docstrings (ëª¨ë“  í´ë˜ìŠ¤)
- [x] ì‚¬ìš© ì˜ˆì œ

### í…ŒìŠ¤íŠ¸

- [x] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ (ê° ì»´í¬ë„ŒíŠ¸)
- [x] í†µí•© í…ŒìŠ¤íŠ¸
- [x] Shape ê²€ì¦
- [x] êµ¬ì¡° ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸

## ğŸ“ˆ ê¸°ëŒ€ íš¨ê³¼

### í•™ìˆ ì  ê¸°ì—¬

1. Transformer + SSM í•˜ì´ë¸Œë¦¬ë“œì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„
2. Stride-based updateë¡œ íš¨ìœ¨ì„± ê·¹ëŒ€í™”
3. Attention poolingì˜ í•™ìŠµ ê°€ëŠ¥í•œ ì••ì¶•

### ì‹¤ìš©ì  ê°€ì¹˜

1. 2300x ê³„ì‚° íš¨ìœ¨ í–¥ìƒ
2. 63x ë©”ëª¨ë¦¬ ì ˆê°
3. Long-context ì²˜ë¦¬ ê°€ëŠ¥
4. Pre-trained ëª¨ë¸ í™œìš©

## ğŸ“ ì°¸ê³  ë¬¸í—Œ

- **Transformer-XL**: Recurrent memory ê°œë…
- **Compressive Transformer**: ì••ì¶• ë©”ëª¨ë¦¬
- **Mamba**: State Space Models
- **RMT**: Memory token ê°œë…
- **Griffin/Hawk**: Hybrid ì•„í‚¤í…ì²˜

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ

1. **ì¦‰ì‹œ ê°€ëŠ¥**:

   - í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (dependencies ì„¤ì¹˜ í•„ìš”)
   - ì½”ë“œ ë¦¬ë·°
   - ì„¤ì • ì¡°ì •

2. **ë‹¨ê¸° (1-2ì£¼)**:

   - Data pipeline êµ¬í˜„
   - Training loop êµ¬í˜„
   - WikiText-103 í•™ìŠµ

3. **ì¤‘ê¸° (1ê°œì›”)**:

   - Pre-trained weights ë¡œë”©
   - Ablation studies
   - ë…¼ë¬¸ ì‘ì„± ì‹œì‘

4. **ì¥ê¸° (2-3ê°œì›”)**:
   - Long-context (PG-19) ì‹¤í—˜
   - ë‹¤ì–‘í•œ downstream tasks
   - ë…¼ë¬¸ ì™„ì„± ë° ì œì¶œ

---

**êµ¬í˜„ ì™„ë£Œì¼**: 2025ë…„ 10ì›” 27ì¼  
**ì´ ì½”ë“œ**: 1,364 lines (60.7 KB)  
**í•µì‹¬ ì»´í¬ë„ŒíŠ¸**: 7ê°œ (ëª¨ë‘ ì™„ì„±)  
**í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€**: í†µí•© í…ŒìŠ¤íŠ¸ í¬í•¨  
**ë¬¸ì„œí™”**: ì™„ë£Œ (README + IMPLEMENTATION)
