# SMT (Stride Memory Transformer) - Quick Start

## ğŸš€ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

### 1. í”„ë¡œì íŠ¸ í™•ì¸

```bash
cd SMT
python scripts/verify_structure.py
```

**ì˜ˆìƒ ì¶œë ¥**:

```
âœ… í”„ë¡œì íŠ¸ êµ¬ì¡° ì¤€ë¹„ ì™„ë£Œ!
ğŸ“Š ì´ ì½”ë“œ í¬ê¸°: 60.7 KB
ğŸ“Š ì´ ì½”ë“œ ë¼ì¸: 1,364 lines
```

### 2. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
pip install -r requirements.txt
```

**í•„ìˆ˜ íŒ¨í‚¤ì§€**:

- `torch>=2.0.0` - PyTorch
- `transformers>=4.30.0` - GPT-2 ëª¨ë¸
- `mamba-ssm>=1.0.0` - Mamba SSM
- `causal-conv1d>=1.0.0` - Mamba dependency

### 3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰

```bash
python tests/test_integration.py
```

**í…ŒìŠ¤íŠ¸ í•­ëª©**:

- âœ… Attention Pooling
- âœ… Window Manager
- âœ… Transformer
- âœ… SSM
- âœ… Full Model

### 4. ì²« ëª¨ë¸ ì‹¤í–‰

```python
from config.model_config import StrideHybridConfig
from src.models.stride_hybrid import StrideHybridModel
import torch

# 1. ì„¤ì • ìƒì„±
config = StrideHybridConfig(
    n_ssm_outputs=15,      # SSM ì¶œë ¥ 15ê°œ
    m_input_tokens=50,     # ì…ë ¥ í† í° 50ê°œ
    stride=16,             # 16 ìŠ¤í…ë§ˆë‹¤ SSM ì—…ë°ì´íŠ¸
    device='cuda'          # or 'cpu'
)

# ì„¤ì • í™•ì¸
config.summary()

# 2. ëª¨ë¸ ìƒì„±
model = StrideHybridModel(config)

# íŒŒë¼ë¯¸í„° í™•ì¸
params = model.count_parameters()
print(f"Total params: {params['total']/1e6:.1f}M")

# 3. Forward pass (í•™ìŠµ)
batch_size = 2
seq_len = 100
input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))

logits, aux = model(input_ids)
print(f"Output shape: {logits.shape}")  # (2, 100, 50280)
print(f"SSM writes: {aux['n_writes']}/{seq_len}")  # 6/100

# 4. Generation (ìƒì„±)
prompt = torch.randint(0, config.vocab_size, (1, 10))  # 10 tokens
generated = model.generate(
    input_ids=prompt,
    max_new_tokens=50,
    temperature=0.8,
    top_p=0.95
)
print(f"Generated shape: {generated.shape}")  # (1, 60)
```

## ğŸ“š í•µì‹¬ ê°œë… ì´í•´í•˜ê¸°

### Window êµ¬ì¡°

```
Step tì˜ window:
[SSM_out[t-14], ..., SSM_out[t],  â† 15ê°œ SSM ì¶œë ¥ (ì••ì¶•ëœ ê³¼ê±°)
 x[t-49], ..., x[t]]              â† 50ê°œ ìµœê·¼ í† í° (ìƒì„¸í•œ ìµœê·¼)
```

### Stride-based Write

```python
for t in range(seq_len):
    # í•­ìƒ: Window ì²˜ë¦¬ â†’ Logits
    window = get_window()
    logits[t] = transformer(window)

    # ì¡°ê±´ë¶€ (16 ìŠ¤í…ë§ˆë‹¤): SSM ì—…ë°ì´íŠ¸
    if t % 16 == 0:
        pooled = attention_pooling(window)  # 65 â†’ 1
        ssm_out = ssm(pooled)               # ì••ì¶• ë©”ëª¨ë¦¬
        add_to_window(ssm_out)
```

### íš¨ìœ¨ì„±

```
Window attention:  O(65Â²) = ~3.2M FLOPs
Full attention:    O(4096Â²) = ~12.9B FLOPs
â†’ 2300x faster! ğŸš€
```

## ğŸ”§ Configuration ê°€ì´ë“œ

### ê¸°ë³¸ ì„¤ì • (WikiText-103)

```python
config = StrideHybridConfig(
    n_ssm_outputs=15,
    m_input_tokens=50,
    stride=16,  # m/3 for 3x coverage

    transformer_n_layers=12,  # GPT-2
    ssm_n_layers=24,          # Mamba-130M

    d_model=768,
    vocab_size=50280,
)
```

### Long-context ì„¤ì • (PG-19)

```python
config = StrideHybridConfig(
    n_ssm_outputs=20,      # ë” ë§ì€ ë©”ëª¨ë¦¬
    m_input_tokens=80,     # ë” ê¸´ ìœˆë„ìš°
    stride=25,             # ë” ê¸´ stride
    # ... ë‚˜ë¨¸ì§€ ë™ì¼
)
```

### ë¹ ë¥¸ ì‹¤í—˜ (Small)

```python
config = StrideHybridConfig(
    n_ssm_outputs=10,
    m_input_tokens=30,
    stride=10,

    transformer_n_layers=6,   # ì‘ì€ ëª¨ë¸
    ssm_n_layers=12,

    d_model=512,
)
```

## ğŸ“Š ëª¨ë¸ ë¶„ì„

### íŒŒë¼ë¯¸í„° í™•ì¸

```python
params = model.count_parameters()

for component, count in params['breakdown'].items():
    print(f"{component:20s}: {count/1e6:6.1f}M")
```

**ì¶œë ¥ ì˜ˆì‹œ**:

```
embedding           :   38.6M
transformer         :  117.0M  â† GPT-2
attention_pooling   :    1.2M  â† ìƒˆë¡œìš´ í•™ìŠµ ê°€ëŠ¥ ë¶€ë¶„
ssm                 :  130.0M  â† Mamba
lm_head            :    0.0M  â† Tied with embedding
```

### Attention íŒ¨í„´ ë¶„ì„

```python
from src.models.components.attention_pooling import AttentionPoolingAnalyzer

# Forward pass
logits, aux = model(input_ids)

# ë¶„ì„
if 'attention_weights' in aux:
    attn_weights = aux['attention_weights'][0]  # ì²« write step

    stats = AttentionPoolingAnalyzer.analyze_attention(
        attn_weights,
        window_size=65,
        n_ssm=15
    )

    print(f"SSM attention:   {stats['ssm_total']:.2%}")
    print(f"Input attention: {stats['input_total']:.2%}")
    print(f"Entropy:         {stats['entropy']:.2f}")

    # Visualization
    AttentionPoolingAnalyzer.visualize_attention(
        attn_weights,
        save_path='attention_pattern.png'
    )
```

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### A. ë°ì´í„° ì¤€ë¹„

```bash
# WikiText-103 ë‹¤ìš´ë¡œë“œ
python -c "from datasets import load_dataset; \
           load_dataset('wikitext', 'wikitext-103-v1')"
```

### B. í•™ìŠµ ì¤€ë¹„

1. Data loader êµ¬í˜„ (`src/data/dataset.py`)
2. Training loop êµ¬í˜„ (`src/training/trainer.py`)
3. Optimizer ì„¤ì • (`src/training/optimizer.py`)

### C. ì‹¤í—˜ ì‹¤í–‰

```bash
# WikiText-103 í•™ìŠµ
python experiments/train_wikitext.py \
    --n_ssm_outputs 15 \
    --m_input_tokens 50 \
    --stride 16 \
    --batch_size 8 \
    --epochs 20
```

## ğŸ› ë¬¸ì œ í•´ê²°

### CUDA Out of Memory

```python
# Batch size ì¤„ì´ê¸°
config = StrideHybridConfig(...)
training_config = TrainingConfig(
    batch_size=2,              # â† 4ì—ì„œ 2ë¡œ
    gradient_accumulation_steps=8  # â† íš¨ê³¼ì  batch size ìœ ì§€
)
```

### mamba-ssm ì„¤ì¹˜ ì‹¤íŒ¨

```bash
# CUDA ì—†ì´ í…ŒìŠ¤íŠ¸
python -c "from src.models.components.ssm import SimpleSSM; \
           print('âœ… Using SimpleSSM fallback')"
```

### transformers ì—†ì´ í…ŒìŠ¤íŠ¸

```python
from src.models.components.transformer import SimpleCausalTransformer
transformer = SimpleCausalTransformer(...)  # GPT-2 ì—†ì´ë„ ì‘ë™
```

## ğŸ“– ì¶”ê°€ ìë£Œ

- **IMPLEMENTATION.md**: ìƒì„¸ êµ¬í˜„ ì„¤ëª… (7.3 KB)
- **SUMMARY.md**: ì™„ì„± ë³´ê³ ì„œ
- **README.md**: í”„ë¡œì íŠ¸ ê°œìš”

## ğŸ’¬ í”¼ë“œë°± & ê¸°ì—¬

ì´ìŠˆë‚˜ ê°œì„  ì‚¬í•­ì´ ìˆìœ¼ì‹œë©´:

1. ì½”ë“œ ë¦¬ë·°
2. í…ŒìŠ¤íŠ¸ ì¶”ê°€
3. ë¬¸ì„œ ê°œì„ 
4. ì‹¤í—˜ ê²°ê³¼ ê³µìœ 

---

**Happy Coding! ğŸš€**

ì´ êµ¬í˜„ì€ PDFì˜ ì•„í‚¤í…ì²˜ë¥¼ ì¶©ì‹¤íˆ ë”°ë¥´ë©´ì„œë„,  
ì—…ë¡œë“œí•˜ì‹  Samba ì½”ë“œì˜ ì‹¤ìš©ì ì¸ íŒ¨í„´ì„ ì¬ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.
