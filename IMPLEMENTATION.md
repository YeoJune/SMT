# Stride-based Hybrid SSM-Transformer - êµ¬í˜„ ì¤€ë¹„ ì™„ë£Œ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
stride_hybrid_ssm/
â”œâ”€â”€ README.md                          âœ… í”„ë¡œì íŠ¸ ì†Œê°œ
â”œâ”€â”€ requirements.txt                   âœ… ì˜ì¡´ì„±
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py                   âœ…
â”‚   â””â”€â”€ model_config.py               âœ… ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py                   âœ…
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py               âœ…
â”‚   â”‚   â”œâ”€â”€ stride_hybrid.py          âœ… ë©”ì¸ ëª¨ë¸
â”‚   â”‚   â”œâ”€â”€ window_manager.py         âœ… ìœˆë„ìš° ê´€ë¦¬
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ __init__.py           âœ…
â”‚   â”‚       â”œâ”€â”€ attention_pooling.py  âœ… Attention Pooling
â”‚   â”‚       â”œâ”€â”€ transformer.py        âœ… Windowed Transformer
â”‚   â”‚       â””â”€â”€ ssm.py                âœ… SSM (Mamba)
â”‚   â”‚
â”‚   â”œâ”€â”€ data/                         ğŸ”² TODO
â”‚   â”œâ”€â”€ training/                     ğŸ”² TODO  
â”‚   â””â”€â”€ utils/                        ğŸ”² TODO
â”‚
â”œâ”€â”€ experiments/                       ğŸ”² TODO
â””â”€â”€ tests/
    â””â”€â”€ test_integration.py           âœ… í†µí•© í…ŒìŠ¤íŠ¸
```

## âœ… êµ¬í˜„ ì™„ë£Œëœ ì»´í¬ë„ŒíŠ¸

### 1. í•µì‹¬ ì•„í‚¤í…ì²˜

#### **AttentionPooling** (`attention_pooling.py`)
- Query-Key-Value attentionìœ¼ë¡œ ìœˆë„ìš° ì••ì¶•
- Query: ìœˆë„ìš° í‰ê· ì˜ projection
- Keys: ê° ìœ„ì¹˜ì˜ projection  
- Output: Attention weightsë¡œ ê°€ì¤‘í•©
- ë¶„ì„ ë„êµ¬ í¬í•¨ (attention pattern visualization)

#### **WindowManager** (`window_manager.py`)
- ë‹¨ì¼/ë°°ì¹˜ ë²„ì „ ëª¨ë‘ êµ¬í˜„
- SSM outputs (nê°œ) + input tokens (mê°œ) ê´€ë¦¬
- Efficient deque ê¸°ë°˜ rotation
- ìƒíƒœ ì €ì¥/ë¡œë“œ ì§€ì›

#### **WindowedTransformer** (`transformer.py`)
- GPT-2 ê¸°ë°˜ (pre-trained weights ë¡œë“œ ê°€ëŠ¥)
- ì‘ì€ ìœˆë„ìš°ë§Œ ì²˜ë¦¬ (65 tokens)
- SimpleCausalTransformerë„ ì œê³µ (pretrained ì—†ì´ í…ŒìŠ¤íŠ¸ìš©)

#### **SSMMemory** (`ssm.py`)
- Mamba ê¸°ë°˜ (pre-trained weights ë¡œë“œ ê°€ëŠ¥)
- 24-layer SSM with RMSNorm
- SimpleSSMë„ ì œê³µ (mamba-ssm ì—†ì´ í…ŒìŠ¤íŠ¸ìš©)

#### **StrideHybridModel** (`stride_hybrid.py`)
- ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©
- Training mode: ì „ì²´ ì‹œí€€ìŠ¤ ì²˜ë¦¬
- Generation mode: auto-regressive ìƒì„±
- íŒŒë¼ë¯¸í„° ì¹´ìš´íŒ… ë° í†µê³„ ì œê³µ

### 2. ì„¤ì • ì‹œìŠ¤í…œ

#### **StrideHybridConfig**
- ëª¨ë“  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜
- ìë™ ê²€ì¦ (stride <= m ë“±)
- ì¶”ì²œ ê°’ ê²½ê³ 
- Pretty print summary

#### **TrainingConfig**
- í•™ìŠµ ê´€ë ¨ ì„¤ì •
- Dataset, optimizer, schedule ë“±

## ğŸ”§ ë‹¤ìŒ êµ¬í˜„ ë‹¨ê³„

### Phase 1: Data Pipeline (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
```python
src/data/
â”œâ”€â”€ dataset.py          # WikiText-103, PG-19 ë¡œë”
â”œâ”€â”€ tokenizer.py        # GPT-2 tokenizer wrapper
â””â”€â”€ data_loader.py      # íš¨ìœ¨ì ì¸ DataLoader
```

### Phase 2: Training Loop
```python
src/training/
â”œâ”€â”€ trainer.py          # ë©”ì¸ í•™ìŠµ ë£¨í”„
â”œâ”€â”€ optimizer.py        # AdamW with grouped LR
â””â”€â”€ scheduler.py        # Cosine annealing
```

### Phase 3: Utilities
```python
src/utils/
â”œâ”€â”€ metrics.py          # Perplexity ê³„ì‚°
â”œâ”€â”€ checkpoint.py       # ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
â””â”€â”€ logging.py          # Wandb í†µí•©
```

### Phase 4: Experiments
```python
experiments/
â”œâ”€â”€ train_wikitext.py   # WikiText-103 í•™ìŠµ
â”œâ”€â”€ train_pg19.py       # PG-19 í•™ìŠµ
â”œâ”€â”€ evaluate.py         # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ ablation/           # Ablation studies
```

## ğŸ¯ í•µì‹¬ ì„¤ê³„ ê²°ì •

### 1. ì—…ë¡œë“œëœ ì½”ë“œì™€ì˜ ì°¨ì´ì 

| íŠ¹ì„± | ì—…ë¡œë“œëœ ì½”ë“œ (Samba) | ì œì•ˆ ì•„í‚¤í…ì²˜ |
|------|---------------------|-------------|
| **ì²˜ë¦¬ ë°©ì‹** | 24ê°œ Mamba ë ˆì´ì–´ ì „ì²´ í†µê³¼ | ì‘ì€ ìœˆë„ìš°ë§Œ Transformer ì²˜ë¦¬ |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©** | ëª¨ë“  ë ˆì´ì–´ ì¶œë ¥ ì €ì¥ | SSM outputsë§Œ ì €ì¥ (n=15) |
| **ì—…ë°ì´íŠ¸** | ë§¤ ìŠ¤í… | Stride ê°„ê²© (6.25%) |
| **ì••ì¶• ë°©ì‹** | LSM ì„ í˜• ë¯¹ì‹± | Attention Pooling |
| **íš¨ìœ¨ì„±** | O(L) per layer | O(windowÂ²) = O(65Â²) |

### 2. ì¬ì‚¬ìš©í•œ íŒ¨í„´

#### âœ… Mamba ì‚¬ìš©ë²•
```python
# From uploaded code
from mamba_ssm import Mamba as MambaCUDA
from mamba_ssm.ops.triton.layer_norm import RMSNorm

# Residual structure
residual = x
x_norm = layer_norm(x)
x_block = mamba_layer(x_norm)
x = residual + x_block
```

#### âœ… GPT-2 ë¡œë”©
```python
# From uploaded code  
from transformers import GPT2Model
model = GPT2Model.from_pretrained("gpt2")
output = model(inputs_embeds=x)  # Direct embedding input
```

#### âœ… Weight Loading íŒ¨í„´
```python
# HuggingFace â†’ Custom model
# 1. Load state dict
# 2. Map keys with prefix replacement
# 3. load_state_dict with strict=False
```

### 3. ë…ì°½ì ì¸ êµ¬í˜„

#### Stride-based Write
```python
if step % stride == 0 and step > 0:
    # Only update 6.25% of steps
    pooled = attention_pooling(window)
    ssm_output = ssm(pooled)
    window_manager.append_ssm(ssm_output)
```

#### Efficient Window Rotation
```python
# deque with maxlen for O(1) rotation
self.ssm_outputs = deque(maxlen=n)
self.input_tokens = deque(maxlen=m)
```

## ğŸ“Š ì„±ëŠ¥ ì˜ˆìƒ

### Computational Efficiency

| Component | Complexity | FLOPs (per step) |
|-----------|-----------|------------------|
| Window Attention | O(wÂ²) | ~3.2M |
| Attention Pooling (amortized) | O(w) | ~2.4M |
| SSM (amortized) | O(dÂ²) | ~0.04M |
| **Total** | - | **~5.6M** |
| Full Attention (L=4096) | O(LÂ²) | ~12.9B |
| **Speedup** | - | **~2300x** |

### Memory Usage

| Item | Size |
|------|------|
| Window activations | 50K floats |
| SSM state | 16 (minimal) |
| KV cache (not used) | 0 |
| **vs Full Attention** | **L/(n+m) = 63x less** |

## ğŸš€ ì‹¤í–‰ ë°©ë²•

### 1. í™˜ê²½ ì„¤ì •
```bash
pip install -r requirements.txt
```

### 2. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
```bash
cd stride_hybrid_ssm
python tests/test_integration.py
```

### 3. ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ
```python
from config.model_config import StrideHybridConfig
from src.models.stride_hybrid import StrideHybridModel

# Create config
config = StrideHybridConfig(
    n_ssm_outputs=15,
    m_input_tokens=50,
    stride=16
)

# Create model
model = StrideHybridModel(config)

# Forward pass
logits, aux = model(input_ids)

# Generate
generated = model.generate(
    input_ids=prompt,
    max_new_tokens=100,
    temperature=0.8
)
```

## ğŸ“ ë‹¤ìŒ í•  ì¼

### ì¦‰ì‹œ ê°€ëŠ¥
1. âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ì—¬ ê¸°ë³¸ ë™ì‘ í™•ì¸
2. ğŸ”² WikiText-103 ë°ì´í„° ë¡œë” êµ¬í˜„
3. ğŸ”² ê¸°ë³¸ í•™ìŠµ ë£¨í”„ ì‘ì„±
4. ğŸ”² Pre-trained weights ë¡œë”© í…ŒìŠ¤íŠ¸

### ì´í›„ ë‹¨ê³„
5. ğŸ”² WikiText-103ì—ì„œ í•™ìŠµ
6. ğŸ”² Perplexity í‰ê°€
7. ğŸ”² Ablation studies (stride, window size)
8. ğŸ”² Long-context (PG-19) ì‹¤í—˜
9. ğŸ”² Needle-in-haystack í…ŒìŠ¤íŠ¸

## ğŸ’¡ ì£¼ìš” ì°¨ë³„ì 

1. **ê³ ì • ìœˆë„ìš° + ë™ì  ì••ì¶•**: ìœˆë„ìš° í¬ê¸°ëŠ” ê³ ì •, ì••ì¶•ì€ í•™ìŠµ ê°€ëŠ¥
2. **Stride ê¸°ë°˜ íš¨ìœ¨ì„±**: 16 ìŠ¤í…ë§ˆë‹¤ 1ë²ˆë§Œ SSM ì—…ë°ì´íŠ¸ (6.25%)
3. **ëª…í™•í•œ ì—­í•  ë¶„ë¦¬**: Transformer=ë‹¨ê¸°, SSM=ì¥ê¸°
4. **êµ¬í˜„ ë‹¨ìˆœì„±**: ~50ì¤„ í•µì‹¬ ë¡œì§
5. **Pre-trained í™œìš©**: GPT-2 + Mamba ì¬ì‚¬ìš©

## ğŸ” ì°¸ê³ ì‚¬í•­

- ëª¨ë“  ì½”ë“œëŠ” GPU/CPU ì–‘ìª½ í˜¸í™˜
- mamba-ssm ì—†ì´ë„ SimpleSSMìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- transformers ì—†ì´ë„ SimpleCausalTransformerë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
- í†µí•© í…ŒìŠ¤íŠ¸ë¡œ ê° ì»´í¬ë„ŒíŠ¸ ê²€ì¦ ì™„ë£Œ
